<table><thead><tr><th>Model</th><th>EN-DE</th><th>BLEU EN-FR</th><th>Training EN-DE</th><th>Cost (FLOPs) EN-FR</th></tr></thead><tbody><tr><td colspan="5">ByteNet [18] 23.75</td></tr><tr><td>Deep-Att + PosUnk [39]</td><td></td><td>39.2</td><td></td><td>1.0 - 102°</td></tr><tr><td>GNMT + RL [38]</td><td>24.6</td><td>39.92</td><td>2.3-10'9</td><td>1.4-10?°</td></tr><tr><td>ConvS2S [9]</td><td>25.16</td><td>40.46</td><td>96-1078</td><td>1.5. 107°</td></tr><tr><td>MoE [32]</td><td>26.03</td><td>40.56</td><td>2.0-10!9</td><td>1.2- 107°</td></tr><tr><td>Deep-Att + PosUnk Ensemble [39]</td><td></td><td>40.4</td><td></td><td>8.0 - 107°</td></tr><tr><td>GNMT + RL Ensemble [38]</td><td>26.30</td><td>41.16</td><td>1.8-1079</td><td>1.1.10?!</td></tr><tr><td>ConvS2S Ensemble [9]</td><td>26.36</td><td>41.29</td><td>7.7-10!9</td><td>1.2.10?!</td></tr><tr><td>Transformer (base model)</td><td>27.3</td><td>38.1</td><td>3.3 -</td><td>1018</td></tr><tr><td>Transformer (big)</td><td>28.4</td><td>41.8</td><td>2.3-</td><td>1019</td></tr></tbody></table>